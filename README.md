# Behavioral Cloning

This project was created as an assessment for the [Self-Driving Car Nanodegree](https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013) Program by Udacity. The goal is to drive a car autonomously in a simulator using a deep neuronal network (DNN) trained on human driving behavior. For that Udacity provided the simulator and a basic python script to connect a DNN with it. The simulator has two mode. In the "training mode" the car can be controlled through a keyboard or a game pad to generated data. More information about the data and it's structure can be found in the corresponding [section](https://github.com/pkern90/behavioral-cloning/blob/master/README.md#data). In the "autonomous mode" however the car receives it input commands by the python script.

The following animations shows the final model controlling the car on both tracks.

Track 1                       |  Track 2
:----------------------------:|:------------------------------:
![Track 1](images/track1.gif) | ![Track 2](images/track2.gif)


# Getting Started
## Prerequisites

This project requires **Python 3.5** and the following Python libraries installed:

- [NumPy](http://www.numpy.org/)
- [SciPy](https://www.scipy.org/)
- [matplotlib](http://matplotlib.org/)
- [pandas](http://pandas.pydata.org/)
- [TensorFlow](http://tensorflow.org)
- [Keras](https://keras.io/)
- [h5py](http://www.h5py.org/)

Only needed for driving in the simulator:

- [flask-socketio](https://flask-socketio.readthedocs.io/en/latest/)
- [eventlet](http://eventlet.net/)
- [pillow](https://python-pillow.org/)


## Run The Drive Script

The drive script needs the path to the model definition as argument. The definition has to be a json file generated by Keras. In addition the model weights have to be located at the same path like the model definition and has to have the same filename (except file type of course). So if all the nessasary files (drive.py, model.json, model.h5) are in the same directory, the script can be executed with the following command:

```
python drive.py model.json
```
The script will automaticaly connect to the simulator and send commands as soon as it's entering the autonomous mode.

## Retrain The Model

To retrain the model it's enough to execute the model.py script without any arguments. Some parameters are set as constants at the beginning of the script and can easily be modified for example to set the path to the training data. An overview of the constants is shown below with the default values.
```python
# Constants
IMG_SIZE = [96, 192]
CROPPING = (32, 0, 0, 0)
SHIFT_OFFSET = 0.2
SHIFT_RANGE = 0.2

BATCH_SIZE = 128
#Patience for early stopping
PATIENCE = 3
# Maximal number of epochs. Might stop earlyer.
NB_EPOCH = 50

TRAINING_DATA_PATHS = ['data/track1_central/driving_log.csv',
                       'data/track1_recovery/driving_log.csv',
                       'data/track1_reverse/driving_log.csv',
                       'data/track1_recovery_reverse/driving_log.csv',
                       'data/track2_central/driving_log.csv']

VALIDATION_DATA_PATHS = ['data/track1_test/driving_log.csv',
                         'data/track2_test/driving_log.csv']
```

# Structure
## Data

During "training mode" the simulator records three images with a frequency of 10hz. Next to a camera centered at the car there are also two additional cameras recording with an offset to the left and right respectively. This allows to apply an approach described in a [paper by Nvidia](https://arxiv.org/abs/1604.07316). A sample of the recorded images is shown in the following table:

Left                                   |  Center                                   |  Right
:-------------------------------------:|:-----------------------------------------:|:-------------------------------------:
![Sample Left](images/left_sample.jpg) | ![Sample Center](images/center_sample.jpg)|![Sample Left](images/right_sample.jpg)

Beside the images the the simulator also creates a log file while recording containing information like the current steering angle, speed and the corresponding image paths. In the displayed image an extract of the log file can be seen, containing all the features.

![Sample Log](images/sample_log.png)

The data used for training the model can be downloaded [here](https://drive.google.com/open?id=0B02X9kiSe3GBczR6MDdscWxuTEU). It contains the following folders:

Used for training

| Name                     | Number Images  | Description                                                               |
|:-------------------------|---------------:|:--------------------------------------------------------------------------|
| track1_central           |  8.978         | driving centered on the road                                              |
| track1_recovery          |  2.369         | driving from the side of the road back to the center                      |
| track1_reverse           |  9.254         | driving as centered on the road as possible in opposite direction         |  
| track1_recovery_reverse  |  2.396         | driving from the side of the road back to the center in opposite direction|
| track2_central           | 19.274         | driving centered on the road on the second track in both directions       |
| **total**                | **42.271**     |                                                                           |


Used for validation

| Name                     | Number Images  | Description                                                               |
|:-------------------------|---------------:|:--------------------------------------------------------------------------|
| track1_test              |  2.882         | driving centered on the road for one round on track 1                     |
| track2_test              |  2.924         | driving centered on the road for one round on track 2                     |
| **total**                | **5.806**      |                                                                           |



## Model

The pretrained model can be optained through the following links:

- [model.json](https://drive.google.com/open?id=0B02X9kiSe3GBOHZUTmU4S0FNck0)
- [model.h5](https://drive.google.com/file/d/0B02X9kiSe3GBTUhkeVFXalQxTlE/view?usp=sharing)


<a href="https://raw.githubusercontent.com/pkern90/behavioral-cloning/master/images/model_wide.png" target="_blank"><img src="images/model_wide.png"></img> </a>

## Training

During training a image generator provides data to the model. Since keras vanilla [ImageDataGenerator](https://keras.io/preprocessing/image/) is mainly suited for classification problems I extended the implementation to better work with continous labels. The two main differnces beeing that flow_from_directory takes the labels as paramter instead of infering them from folder names and ability to add transform funktion for the labels to the varius random image transformations. The latter allows to generate randomly transforme images with modified expected values. One particular usecase is to randomly flip road images. If a image gets flipped you also need to change sign of the steering angle. Other changes include the option to pass a function as rescale parameter and the option to cropp images.

The following code snipped shows an example usage of the modified ImageDataGenerator. Images will be normalized to a range from -1 to 1, randomly flipped and also cropped from the top by 32 pixel.

```python
datagen = RegressionImageDataGenerator(rescale=lambda x: x / 127.5 - 1.,
                                       horizontal_flip=True,
                                       horizontal_flip_value_transform=lambda val: -val,
                                       cropping=(32, 0, 0, 0))
```
